// parser.v creates the AST structure from the tokens generated by lexer.v

module vsql

struct Parser {
	tokens []Token
mut:
	pos int
}

fn parse(sql string) ?Stmt {
	tokens := tokenize(sql)
	mut parser := Parser{tokens, 0}
	mut stmt := Stmt(SelectStmt{})

	match tokens[0].kind {
		.keyword_create {
			stmt = parser.consume_create_table() or { return err }
		}
		.keyword_delete {
			stmt = parser.consume_delete() or { return err }
		}
		.keyword_drop {
			stmt = parser.consume_drop_table() or { return err }
		}
		.keyword_insert {
			stmt = parser.consume_insert() or { return err }
		}
		.keyword_select {
			stmt = parser.consume_select() or { return err }
		}
		.keyword_update {
			stmt = parser.consume_update() or { return err }
		}
		else {
			return sqlstate_42601('at "${tokens[0].value}"') // syntax error
		}
	}

	// The ; is optional. However, we do not support multiple queries yet so
	// make sure we catch that.
	if parser.peek(.semicolon).len > 0 {
		parser.pos++
	}

	if tokens[parser.pos].kind != .eof {
		return sqlstate_42601('at "${tokens[parser.pos].value}"') // syntax error
	}

	return stmt
}

fn (mut p Parser) peek(tks ...TokenKind) []Token {
	mut pos := p.pos
	mut toks := []Token{}

	for tk in tks {
		if p.tokens[pos].kind != tk {
			return []Token{}
		}

		toks << p.tokens[pos]
		pos++
	}

	return toks
}

fn (mut p Parser) consume_type() ?Type {
	// These need to be sorted with longest first to avoid consuming an
	// incomplete type.
	types := [
		// 5
		[TokenKind.keyword_char, .keyword_varying, .left_paren, .literal_number, .right_paren],
		[.keyword_character, .keyword_varying, .left_paren, .literal_number, .right_paren],
		// 4
		[.keyword_char, .left_paren, .literal_number, .right_paren],
		[.keyword_character, .left_paren, .literal_number, .right_paren],
		[.keyword_float, .left_paren, .literal_number, .right_paren],
		[.keyword_varchar, .left_paren, .literal_number, .right_paren],
		// 2
		[.keyword_double, .keyword_precision],
		// 1
		[.keyword_bigint],
		[.keyword_boolean],
		[.keyword_character],
		[.keyword_char],
		[.keyword_float],
		[.keyword_integer],
		[.keyword_int],
		[.keyword_real],
		[.keyword_smallint],
	]
	for typ in types {
		peek := p.peek(...typ)
		if peek.len > 0 {
			p.pos += peek.len

			mut type_name := ''
			mut type_size := '0'
			match typ.len {
				1 {
					type_name = peek[0].value
				}
				2 {
					type_name = '${peek[0].value} ${peek[1].value}'
				}
				4 {
					type_name = peek[0].value
					type_size = peek[3].value
				}
				5 {
					type_name = '${peek[0].value} ${peek[1].value}'
					type_size = peek[3].value
				}
				else {
					panic(peek)
				}
			}

			return new_type(type_name.to_upper(), type_size.int())
		}
	}

	return sqlstate_42601('expecting type but found ${p.tokens[p.pos].value}')
}

fn (mut p Parser) consume_create_table() ?CreateTableStmt {
	// CREATE TABLE <table_name>
	p.consume(.keyword_create) ?
	p.consume(.keyword_table) ?
	table_name := p.consume(.literal_identifier) ?

	// columns
	p.consume(.left_paren) ?

	mut columns := []Column{}
	columns << p.consume_column_def() ?

	for p.peek(.comma).len > 0 {
		p.consume(.comma) ?
		columns << p.consume_column_def() ?
	}

	p.consume(.right_paren) ?

	return CreateTableStmt{table_name.value, columns}
}

fn (mut p Parser) consume_column_def() ?Column {
	col_name := p.consume(.literal_identifier) ?
	col_type := p.consume_type() ?

	mut not_null := false
	if p.peek(.keyword_not, .keyword_null).len > 0 {
		p.pos += 2
		not_null = true
	} else if p.peek(.keyword_null).len > 0 {
		p.pos++
	}

	return Column{col_name.value, col_type, not_null}
}

fn (mut p Parser) consume(tk TokenKind) ?Token {
	if p.tokens[p.pos].kind == tk {
		defer {
			p.pos++
		}

		return p.tokens[p.pos]
	}

	return sqlstate_42601('expecting $tk but found ${p.tokens[p.pos].value}')
}

fn (mut p Parser) consume_insert() ?InsertStmt {
	// INSERT INTO <table_name>
	p.consume(.keyword_insert) ?
	p.consume(.keyword_into) ?
	table_name := p.consume(.literal_identifier) ?

	// columns
	mut cols := []string{}
	p.consume(.left_paren) ?
	col := p.consume(.literal_identifier) ?
	cols << col.value

	for p.peek(.comma).len > 0 {
		p.pos++
		next_col := p.consume(.literal_identifier) ?
		cols << next_col.value
	}

	p.consume(.right_paren) ?

	// values
	mut values := []Value{}
	p.consume(.keyword_values) ?
	p.consume(.left_paren) ?
	values << p.consume_value() ?

	for p.peek(.comma).len > 0 {
		p.pos++
		values << p.consume_value() ?
	}

	p.consume(.right_paren) ?

	return InsertStmt{table_name.value, cols, values}
}

fn (mut p Parser) consume_grouping_expr() ?Expr {
	start := p.pos

	p.consume(.left_paren) or {
		p.pos = start
		return err
	}

	expr := p.consume_expr() or {
		p.pos = start
		return err
	}

	p.consume(.right_paren) or {
		p.pos = start
		return err
	}

	return expr
}

fn (mut p Parser) consume_select() ?SelectStmt {
	// skip SELECT
	p.pos++

	// expressions
	mut exprs := []Expr{}
	exprs << p.consume_expr() ?

	for p.peek(.comma).len > 0 {
		p.pos++ // skip ','
		exprs << p.consume_expr() ?
	}

	// FROM
	mut from := ''
	if p.tokens[p.pos].kind == .keyword_from {
		from = p.tokens[p.pos + 1].value
		p.pos += 2
	}

	// WHERE
	mut where := Expr(NoExpr{})
	if p.peek(.keyword_where).len > 0 {
		p.pos++ // skip WHERE
		where = p.consume_expr() ?
	}

	return SelectStmt{exprs, from, where}
}

fn (mut p Parser) consume_drop_table() ?DropTableStmt {
	// DROP TABLE <table_name>
	p.consume(.keyword_drop) ?
	p.consume(.keyword_table) ?
	table_name := p.consume(.literal_identifier) ?

	return DropTableStmt{table_name.value}
}

fn (mut p Parser) consume_delete() ?DeleteStmt {
	// DELETE FROM <table_name>
	p.consume(.keyword_delete) ?
	p.consume(.keyword_from) ?
	table_name := p.consume(.literal_identifier) ?

	// WHERE
	mut expr := Expr(NoExpr{})
	if p.peek(.keyword_where).len > 0 {
		p.pos++ // skip WHERE
		expr = p.consume_expr() ?
	}

	return DeleteStmt{table_name.value, expr}
}

fn (mut p Parser) consume_expr() ?Expr {
	// TODO(elliotchance): This should not be allowed outside of SELECT
	// expressions and this returns a dummy value for now.
	if p.peek(.asterisk).len > 0 {
		p.pos++
		return new_null_value()
	}

	allowed_ops := [
		TokenKind.asterisk,
		.concatenation_operator,
		.equals_operator,
		.greater_than_operator,
		.greater_than_or_equals_operator,
		.keyword_and,
		.keyword_or,
		.less_than_operator,
		.less_than_or_equals_operator,
		.minus_sign,
		.not_equals_operator,
		.plus_sign,
		.solidus,
	]
	mut parts := [p.consume_simple_expr() ?]
	mut operators := []Token{}
	for {
		if p.peek(.keyword_is, .keyword_null).len > 0 {
			p.pos += 2
			parts[parts.len - 1] = NullExpr{parts[parts.len - 1], false}
		} else if p.peek(.keyword_is, .keyword_not, .keyword_null).len > 0 {
			p.pos += 3
			parts[parts.len - 1] = NullExpr{parts[parts.len - 1], true}
		} else {
			if p.tokens[p.pos].kind in allowed_ops {
				operators << p.tokens[p.pos]
				p.pos++

				parts << p.consume_simple_expr() ?
			} else {
				break
			}
		}
	}

	return expr_precedence(parts, operators)
}

fn expr_precedence(parts []Expr, operators []Token) Expr {
	if parts.len == 1 {
		return parts[0]
	}

	if parts.len == 2 {
		return BinaryExpr{parts[0], operators[0].value, parts[1]}
	}

	mut lowest := precedence(operators[0].kind)
	mut at := 0
	mut i := 1
	for i < operators.len {
		p := precedence(operators[i].kind)
		if p < lowest {
			lowest = p
			at = i
		}
		i++
	}

	return BinaryExpr{expr_precedence(parts[..at], operators[..at - 1]), operators[at - 1].value, expr_precedence(parts[at..],
		operators[at..])}
}

fn (mut p Parser) consume_identifier() ?Identifier {
	if p.peek(.literal_identifier).len > 0 {
		p.pos++
		return Identifier{p.tokens[p.pos - 1].value}
	}

	return sqlstate_42601('expecting identifier but found ${p.tokens[p.pos].value}')
}

fn (mut p Parser) consume_unary_expr() ?Expr {
	start := p.pos

	ops := [TokenKind.minus_sign, .plus_sign, .keyword_not]
	for op in ops {
		if p.peek(op).len > 0 {
			p.pos++
			return UnaryExpr{p.tokens[p.pos - 2].value, p.consume_value() or {
				p.pos = start
				return err
			}}
		}
	}

	p.pos = start
	return error('expecting unary')
}

fn (mut p Parser) consume_simple_expr() ?Expr {
	start := p.pos
	return p.consume_grouping_expr() or {
		return p.consume_unary_expr() or {
			return p.consume_identifier() or {
				value := p.consume_value() or {
					p.pos = start
					return err
				}

				return value
			}
		}
	}
}

fn (mut p Parser) consume_value() ?Value {
	if p.peek(.keyword_null).len > 0 {
		p.pos++
		return new_null_value()
	}

	if p.peek(.keyword_true).len > 0 {
		p.pos++
		return new_boolean_value(true)
	}

	if p.peek(.keyword_false).len > 0 {
		p.pos++
		return new_boolean_value(false)
	}

	if p.peek(.keyword_unknown).len > 0 {
		p.pos++
		return new_unknown_value()
	}

	if p.peek(.literal_number).len > 0 {
		t := p.consume(.literal_number) ?
		if t.value.contains('.') {
			return new_float_value(t.value.f64())
		}

		return new_integer_value(t.value.int())
	}

	if p.peek(.literal_string).len > 0 {
		t := p.consume(.literal_string) ?
		return new_varchar_value(t.value, 0)
	}

	return sqlstate_42601('expecting value but found ${p.tokens[p.pos].value}')
}

fn (mut p Parser) consume_update() ?UpdateStmt {
	// UPDATE <table_name>
	p.consume(.keyword_update) ?
	table_name := p.consume(.literal_identifier) ?

	// SET
	p.consume(.keyword_set) ?
	col_name := p.consume(.literal_identifier) ?
	p.consume(.equals_operator) ?
	col_value := p.consume_value() ?
	mut set := map[string]Value{}
	set[col_name.value] = col_value

	// WHERE
	mut expr := Expr(NoExpr{})
	if p.peek(.keyword_where).len > 0 {
		p.pos++ // skip WHERE
		expr = p.consume_expr() ?
	}

	return UpdateStmt{table_name.value, set, expr}
}
