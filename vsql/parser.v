// parser.v creates the AST structure from the tokens generated by lexer.v

module vsql

struct Parser {
	tokens []Token
mut:
	pos int
}

fn parse(sql string) ?Stmt {
	tokens := tokenize(sql)
	mut parser := Parser{tokens, 0}
	mut stmt := Stmt(SelectStmt{})

	match tokens[0].kind {
		.keyword_create {
			stmt = parser.consume_create_table() or { return err }
		}
		.keyword_delete {
			stmt = parser.consume_delete() or { return err }
		}
		.keyword_drop {
			stmt = parser.consume_drop_table() or { return err }
		}
		.keyword_insert {
			stmt = parser.consume_insert() or { return err }
		}
		.keyword_select {
			stmt = parser.consume_select() or { return err }
		}
		.keyword_update {
			stmt = parser.consume_update() or { return err }
		}
		else {
			return sqlstate_42601('at "${tokens[0].value}"') // syntax error
		}
	}

	// The ; is optional. However, we do not support multiple queries yet so
	// make sure we catch that.
	if parser.peek(.op_semi_colon).len > 0 {
		parser.pos++
	}

	if tokens[parser.pos].kind != .eof {
		return sqlstate_42601('at "${tokens[parser.pos].value}"') // syntax error
	}

	return stmt
}

fn (mut p Parser) peek(tks ...TokenKind) []Token {
	mut pos := p.pos
	mut toks := []Token{}

	for tk in tks {
		if p.tokens[pos].kind != tk {
			return []Token{}
		}

		toks << p.tokens[pos]
		pos++
	}

	return toks
}

fn (mut p Parser) consume_type() ?Type {
	// These need to be sorted with longest first to avoid consuming an
	// incomplete type.
	types := [
		// 5
		[TokenKind.keyword_char, .keyword_varying, .op_paren_open, .literal_number, .op_paren_close],
		[.keyword_character, .keyword_varying, .op_paren_open, .literal_number, .op_paren_close],
		// 4
		[.keyword_char, .op_paren_open, .literal_number, .op_paren_close],
		[.keyword_character, .op_paren_open, .literal_number, .op_paren_close],
		[.keyword_float, .op_paren_open, .literal_number, .op_paren_close],
		[.keyword_varchar, .op_paren_open, .literal_number, .op_paren_close],
		// 2
		[.keyword_double, .keyword_precision],
		// 1
		[.keyword_bigint],
		[.keyword_boolean],
		[.keyword_character],
		[.keyword_char],
		[.keyword_float],
		[.keyword_integer],
		[.keyword_int],
		[.keyword_real],
		[.keyword_smallint],
	]
	for typ in types {
		peek := p.peek(...typ)
		if peek.len > 0 {
			p.pos += peek.len

			mut type_name := ''
			mut type_size := '0'
			match typ.len {
				1 {
					type_name = peek[0].value
				}
				2 {
					type_name = '${peek[0].value} ${peek[1].value}'
				}
				4 {
					type_name = peek[0].value
					type_size = peek[3].value
				}
				5 {
					type_name = '${peek[0].value} ${peek[1].value}'
					type_size = peek[3].value
				}
				else {
					panic(peek)
				}
			}

			return new_type(type_name.to_upper(), type_size.int())
		}
	}

	return sqlstate_42601('expecting type but found ${p.tokens[p.pos].value}')
}

fn (mut p Parser) consume_create_table() ?CreateTableStmt {
	// CREATE TABLE <table_name>
	p.consume(.keyword_create) ?
	p.consume(.keyword_table) ?
	table_name := p.consume(.literal_identifier) ?

	// columns
	p.consume(.op_paren_open) ?

	mut columns := []Column{}
	columns << p.consume_column_def() ?

	for p.peek(.op_comma).len > 0 {
		p.consume(.op_comma) ?
		columns << p.consume_column_def() ?
	}

	p.consume(.op_paren_close) ?

	return CreateTableStmt{table_name.value, columns}
}

fn (mut p Parser) consume_column_def() ?Column {
	col_name := p.consume(.literal_identifier) ?
	col_type := p.consume_type() ?

	mut not_null := false
	if p.peek(.keyword_not, .keyword_null).len > 0 {
		p.pos += 2
		not_null = true
	} else if p.peek(.keyword_null).len > 0 {
		p.pos++
	}

	return Column{col_name.value, col_type, not_null}
}

fn (mut p Parser) consume(tk TokenKind) ?Token {
	if p.tokens[p.pos].kind == tk {
		defer {
			p.pos++
		}

		return p.tokens[p.pos]
	}

	return sqlstate_42601('expecting $tk but found ${p.tokens[p.pos].value}')
}

fn (mut p Parser) consume_insert() ?InsertStmt {
	// INSERT INTO <table_name>
	p.consume(.keyword_insert) ?
	p.consume(.keyword_into) ?
	table_name := p.consume(.literal_identifier) ?

	// columns
	mut cols := []string{}
	p.consume(.op_paren_open) ?
	col := p.consume(.literal_identifier) ?
	cols << col.value

	for p.peek(.op_comma).len > 0 {
		p.pos++
		next_col := p.consume(.literal_identifier) ?
		cols << next_col.value
	}

	p.consume(.op_paren_close) ?

	// values
	mut values := []Value{}
	p.consume(.keyword_values) ?
	p.consume(.op_paren_open) ?
	values << p.consume_value() ?

	for p.peek(.op_comma).len > 0 {
		p.pos++
		values << p.consume_value() ?
	}

	p.consume(.op_paren_close) ?

	return InsertStmt{table_name.value, cols, values}
}

fn (mut p Parser) consume_select() ?SelectStmt {
	// skip SELECT
	p.pos++

	// expressions
	mut exprs := []Expr{}
	exprs << p.consume_expr() ?

	for p.peek(.op_comma).len > 0 {
		p.pos++ // skip ','
		exprs << p.consume_expr() ?
	}

	// FROM
	mut from := ''
	if p.tokens[p.pos].kind == .keyword_from {
		from = p.tokens[p.pos + 1].value
		p.pos += 2
	}

	// WHERE
	mut where := Expr(NoExpr{})
	if p.peek(.keyword_where).len > 0 {
		p.pos++ // skip WHERE
		where = p.consume_expr() ?
	}

	return SelectStmt{exprs, from, where}
}

fn (mut p Parser) consume_drop_table() ?DropTableStmt {
	// DROP TABLE <table_name>
	p.consume(.keyword_drop) ?
	p.consume(.keyword_table) ?
	table_name := p.consume(.literal_identifier) ?

	return DropTableStmt{table_name.value}
}

fn (mut p Parser) consume_delete() ?DeleteStmt {
	// DELETE FROM <table_name>
	p.consume(.keyword_delete) ?
	p.consume(.keyword_from) ?
	table_name := p.consume(.literal_identifier) ?

	// WHERE
	mut expr := Expr(NoExpr{})
	if p.peek(.keyword_where).len > 0 {
		p.pos++ // skip WHERE
		expr = p.consume_expr() ?
	}

	return DeleteStmt{table_name.value, expr}
}

fn (mut p Parser) consume_expr() ?Expr {
	// TODO(elliotchance): This should not be allowed outside of SELECT
	// expressions and this returns a dummy value for now.
	if p.peek(.op_multiply).len > 0 {
		p.pos++
		return new_null_value()
	}

	return p.consume_binary_expr() or {
		return p.consume_null_expr() or {
			// Value (must be last).
			value := p.consume_value() ?
			return value
		}
	}
}

fn (mut p Parser) consume_identifier() ?Identifier {
	if p.peek(.literal_identifier).len > 0 {
		p.pos++
		return Identifier{p.tokens[p.pos - 1].value}
	}

	return sqlstate_42601('expecting identifier but found ${p.tokens[p.pos].value}')
}

fn (mut p Parser) consume_value_or_identifier() ?Expr {
	return p.consume_identifier() or {
		value := p.consume_value() ?
		return value
	}
}

fn (mut p Parser) consume_null_expr() ?NullExpr {
	start := p.pos

	expr := p.consume_value_or_identifier() or {
		p.pos = start
		return sqlstate_42601('expecting expr but found ${p.tokens[p.pos].value}')
	}

	if p.peek(.keyword_is, .keyword_null).len > 0 {
		p.pos += 2
		return NullExpr{expr, false}
	}

	if p.peek(.keyword_is, .keyword_not, .keyword_null).len > 0 {
		p.pos += 3
		return NullExpr{expr, true}
	}

	p.pos = start
	return sqlstate_42601('expecting null expr but found ${p.tokens[p.pos].value}')
}

fn (mut p Parser) consume_binary_expr() ?BinaryExpr {
	start := p.pos

	lhs := p.consume(.literal_identifier) or {
		p.pos = start
		return err
	}
	mut op := Token{}

	allowed_ops := [
		TokenKind.op_eq,
		.op_neq,
		.op_gt,
		.op_gte,
		.op_lt,
		.op_lte,
	]
	for allowed_op in allowed_ops {
		if p.peek(allowed_op).len > 0 {
			op = p.consume(allowed_op) or {
				p.pos = start
				return err
			}
			break
		}
	}

	rhs := p.consume_value() or {
		p.pos = start
		return err
	}

	return BinaryExpr{lhs.value, op.value, rhs}
}

fn (mut p Parser) consume_value() ?Value {
	if p.peek(.keyword_null).len > 0 {
		p.pos++
		return new_null_value()
	}

	if p.peek(.keyword_true).len > 0 {
		p.pos++
		return new_boolean_value(true)
	}

	if p.peek(.keyword_false).len > 0 {
		p.pos++
		return new_boolean_value(false)
	}

	if p.peek(.keyword_unknown).len > 0 {
		p.pos++
		return new_unknown_value()
	}

	if p.peek(.literal_number).len > 0 {
		t := p.consume(.literal_number) ?
		if t.value.contains('.') {
			return new_float_value(t.value.f64())
		}

		return new_integer_value(t.value.int())
	}

	if p.peek(.literal_string).len > 0 {
		t := p.consume(.literal_string) ?
		return new_varchar_value(t.value, 0)
	}

	return sqlstate_42601('expecting value but found ${p.tokens[p.pos].value}')
}

fn (mut p Parser) consume_update() ?UpdateStmt {
	// UPDATE <table_name>
	p.consume(.keyword_update) ?
	table_name := p.consume(.literal_identifier) ?

	// SET
	p.consume(.keyword_set) ?
	col_name := p.consume(.literal_identifier) ?
	p.consume(.op_eq) ?
	col_value := p.consume_value() ?
	mut set := map[string]Value{}
	set[col_name.value] = col_value

	// WHERE
	mut expr := Expr(NoExpr{})
	if p.peek(.keyword_where).len > 0 {
		p.pos++ // skip WHERE
		expr = p.consume_expr() ?
	}

	return UpdateStmt{table_name.value, set, expr}
}
